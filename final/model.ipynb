{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cooltools\n",
    "import cooler\n",
    "import cooltools.lib.plotting\n",
    "from matplotlib.ticker import EngFormatter\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import csv\n",
    "import pysam \n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=torch.load(\"./data/train.pth\"),torch.load(\"./data/test.pth\")\n",
    "train_loader,test_loader= DataLoader(train, batch_size=5,shuffle=True),DataLoader(test, batch_size=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hic(x,name=\"hello\"):\n",
    "    mat=torch.zeros(448,448)\n",
    "    idx=torch.triu_indices(448, 448,2)\n",
    "    mat[idx[0],idx[1]]=x.float()\n",
    "    im=plt.imshow(mat+mat.T,vmax=1)\n",
    "    plt.colorbar(im ,fraction=0.046, pad=0.04, label='balanced')\n",
    "    plt.savefig(f\"{name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class Conv1D_block(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels,kernel_size=11,dilation=10,m_kernel=2):\n",
    "    super().__init__()\n",
    "    padding=dilation*(kernel_size-1)//2\n",
    "    self.conv1d=nn.Conv1d(in_channels, out_channels,kernel_size,padding=padding,dilation=dilation)\n",
    "    self.relu1=nn.ReLU()\n",
    "    self.bn=nn.BatchNorm1d(out_channels)\n",
    "    self.pool=nn.MaxPool1d(m_kernel,stride=2)\n",
    "    self.Seq=nn.Sequential(self.relu1,self.conv1d,self.bn,self.pool)\n",
    "  def __call__(self,x):\n",
    "    return self.Seq(x)\n",
    "\n",
    "class ResNet1d(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels,kernel_size=5,dilation=10):\n",
    "    super().__init__()\n",
    "    padding=dilation*(kernel_size-1)//2\n",
    "    self.conv1d1,self.conv1d2=nn.Conv1d(in_channels, out_channels//2,kernel_size,padding=padding,dilation=dilation),nn.Conv1d(out_channels//2, in_channels,kernel_size,padding=padding,dilation=dilation)\n",
    "    self.relu1,self.relu2=nn.ReLU(),nn.ReLU()\n",
    "    self.Dropout=nn.Dropout()\n",
    "    self.bn1,self.bn2=nn.BatchNorm1d(out_channels//2),nn.BatchNorm1d(in_channels)\n",
    "    self.Seq=nn.Sequential(self.relu1,self.conv1d1,self.bn1,self.relu2,self.conv1d2,self.bn2,self.Dropout)\n",
    "  def __call__(self,x):\n",
    "    y=x\n",
    "    return self.Seq(y)+x\n",
    "class one_two(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels,mid_channel=64,kernel_size=5,dilation=10,dim=512,device=\"cpu\"):\n",
    "    super().__init__()\n",
    "    padding=dilation*(kernel_size-1)//2\n",
    "    self.conv1d=nn.Conv1d(in_channels, mid_channel ,kernel_size,padding=padding,dilation=dilation)\n",
    "    self.relu1,self.relu2,self.relu3=nn.ReLU(),nn.ReLU(),nn.ReLU()\n",
    "    self.bn,self.bn1=nn.BatchNorm1d(mid_channel),nn.BatchNorm2d(out_channels)\n",
    "    self.Seq=nn.Sequential(self.relu1,self.conv1d,self.bn,self.relu2)\n",
    "    self.conv2d,self.conv2d1=nn.Conv2d(in_channels, mid_channel+1,1),nn.Conv2d(mid_channel+1,out_channels ,kernel_size=5,padding=2)\n",
    "    self.sem=symmetrize2d()\n",
    "    self.Seq2=nn.Sequential(self.relu3,self.conv2d1,self.bn1,self.sem)\n",
    "    l=dim\n",
    "    a=torch.tensor([list(range(l))])\n",
    "    b=a.mT\n",
    "    self.dist=torch.abs(b-a).unsqueeze(0).unsqueeze(0)\n",
    "    self.dist=self.dist.to(device)\n",
    "  def __call__(self, x):\n",
    "    x=self.Seq(x)\n",
    "    y=torch.cat([torch.add(x.unsqueeze(-1),torch.transpose(x.unsqueeze(-1), -2, -1))/2,self.dist.repeat(x.shape[0],1,1,1)],1)\n",
    "    return self.Seq2(y)\n",
    "\n",
    "class conv2d_block(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels,kernel_size,dilation=5):\n",
    "    super().__init__()\n",
    "    padding=dilation*(kernel_size-1)//2\n",
    "    self.conv2d=nn.Conv1d(in_channels, out_channels,kernel_size,padding=padding,dilation=dilation)\n",
    "    self.relu=nn.ReLU()\n",
    "    self.bn1=nn.BatchNorm2d(out_channels)\n",
    "    self.Seq=nn.Sequential(self.relu1,self.conv2d,self.bn1)\n",
    "\n",
    "  def __call__(self,x):\n",
    "    x=self.Seq(x)\n",
    "    return x\n",
    "class symmetrize2d(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  def __call__(self,x):\n",
    "    return (x+x.transpose(-1,-2))/2\n",
    "\n",
    "class ResNet2d(nn.Module):\n",
    "  def __init__(self,in_channels,out_channels,kernel_size=3,dilation=5):\n",
    "    super().__init__()\n",
    "    padding=dilation*(kernel_size-1)//2\n",
    "    self.conv2d1,self.conv2d2=nn.Conv2d(in_channels, out_channels//2,kernel_size,padding=padding,dilation=dilation),nn.Conv2d(out_channels//2, in_channels,kernel_size,padding=padding,dilation=dilation)\n",
    "    self.relu1,self.relu2=nn.ReLU(),nn.ReLU()\n",
    "    self.Dropout=nn.Dropout()\n",
    "    self.bn1,self.bn2=nn.BatchNorm2d(out_channels//2),nn.BatchNorm2d(in_channels)\n",
    "    self.Seq=nn.Sequential(self.relu1,self.conv2d1,self.bn1,self.relu2,self.conv2d2,self.bn2,self.Dropout)\n",
    "  def __call__(self,x):\n",
    "    y=x\n",
    "    return self.Seq(y)+x\n",
    "class Crop2D(nn.Module):\n",
    "  def __init__(self,length):\n",
    "    super().__init__()\n",
    "    self.length=length\n",
    "  def __call__(self,x):\n",
    "    return x[:,:,self.length:-self.length,self.length:-self.length]\n",
    "class Upper_triangle(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  def __call__(self,x):\n",
    "    return x[:,:,torch.triu(torch.ones(x.shape[-1], x.shape[-1]),2) == 1]\n",
    "class FC(nn.Module):\n",
    "  def __init__(self,in_f,out_f=5):\n",
    "    super().__init__()\n",
    "    self.fc=nn.Linear(in_f,out_f)\n",
    "  def __call__(self,x):\n",
    "    return self.fc(x.transpose(-2,-1))\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self,in_channels,conv1D_block=11,resNet1d=5,resNet2d=5,out=1,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    super().__init__()\n",
    "    layers=[Conv1D_block(in_channels,96)]\n",
    "    for i in range(conv1D_block-1):\n",
    "      layers.append(Conv1D_block(96,96))\n",
    "    for i in range(resNet1d):\n",
    "      layers.append(ResNet1d(96,96))\n",
    "    layers.append(one_two(96,48,device=device))\n",
    "    for i in range(resNet2d):\n",
    "      layers.append(ResNet2d(48,48))\n",
    "      layers.append(symmetrize2d())\n",
    "    layers.append(Crop2D(32))\n",
    "    layers.append(Upper_triangle())\n",
    "    layers.append(FC(48,out))\n",
    "    self.layers=layers\n",
    "    self.Seq=nn.Sequential(*self.layers)\n",
    "  def __call__(self,x):\n",
    "    # for i in self.layers:\n",
    "    #   print(x.shape)\n",
    "    #   x=i(x)\n",
    "    return self.Seq(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, criterion,train_loader,epochs,root=\"./\",model_name=\"Akita\", plot=False,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    model = net.to(device)\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    overall_step = 0\n",
    "    train_loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            # Move tensors to configured device\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            #Forward Pass\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            total += y.size(0)\n",
    "            optimizer.step()\n",
    "            if (i+1) % 4 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "            if plot:\n",
    "                info = { ('loss_' + model_name): loss.item() }\n",
    "        train_loss_values.append(running_loss/total)\n",
    "        if running_loss/total<1e-3 : break\n",
    "    return train_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,criterion,test_loader,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    model = net.to(device)\n",
    "    model.eval()   \n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        running_loss=0.0\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss += loss.item()\n",
    "            total += y.size(0)\n",
    "            \n",
    "        print('Accuracy of the network on the test: {}'.format(running_loss/total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Model(5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "epochs=1\n",
    "#net, optimizer, criterion,filename,epochs\n",
    "train_loss_values=train(net, optimizer, criterion,train_loader, epochs,model_name=\"Akita\")\n",
    "torch.save(net.state_dict(), \"./model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_loss_values))),train_loss_values,label=\"training error\",color=\"red\")\n",
    "plt.legend()\n",
    "plt.savefig(\"trainingcurve.png\")\n",
    "plt.show()\n",
    "print(train_loss_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net,criterion,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y in test_loader:\n",
    "    X=X.to(device).float()\n",
    "    pred=net(X).detach()\n",
    "    plot_hic(pred[0,:,0],\"1\")\n",
    "    plot_hic(y[0,:,0],\"1\")\n",
    "    plot_hic(pred[1,:,0],\"2\")\n",
    "    plot_hic(y[1,:,0],\"2\")\n",
    "    plot_hic(pred[2,:,0],\"3\")\n",
    "    plot_hic(y[2,:,0],\"3\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
